{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cee869-9d96-4320-8745-2918f8666188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login, login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../finetune/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5250f8-dc34-4eb3-99e7-e5fa5f251801",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2000f7be-d2ff-474f-9bc4-7378d64383d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d087eb4a6bef403e85ba67336e414562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5483d5-9f69-4e2d-9e1e-52ef4a7de5e0",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8259f4c-4baa-44ae-b60b-2dabe4b4c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14cfc702-d836-4b74-8e55-953924106b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d91d451ddc4bcca7b056897475353c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# load model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.pretraining_tp = 1 #parallel GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bebeb01-71e6-4476-ad96-4c580aa33a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "                                          add_eos_token = True, \n",
    "                                          add_bos_token = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # default is none\n",
    "tokenizer.eos_token_id # for attention mask? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519c0ac-5701-4a69-8d84-91690a0416fe",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8ffe7a-8201-4bde-a6ca-75cb41db7edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Context', 'Answer'],\n",
       "        num_rows: 348\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"../data/sub_finetune_2.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15030b0b-f338-487d-bf06-5e8ee120b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_row(question, context, answer):\n",
    "    return f\"\"\"<s>### Instruction:\\n{question}\\n### Context: \\n{context}\\n### Response: {answer}</s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d6c5f5-b7ef-4243-932f-e58ce4a788d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(df):\n",
    "    questions = df[\"Question\"]\n",
    "    contexts = df[\"Context\"]\n",
    "    answers = df[\"Answer\"]\n",
    "    texts = []\n",
    "    for q, c, a in zip(questions, contexts, answers):\n",
    "        text = create_text_row(q, c, a)\n",
    "        texts.append(text)\n",
    "    return {\"text\" : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3575e432-ab90-4cc3-aed2-b19dc1231e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee7265c7-a901-48df-8768-fc7034d1b734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Context', 'Answer', 'text'],\n",
       "        num_rows: 348\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4453245a-5b50-46ef-b2da-2e5da3c8b645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Assume you are a chess master, explain the strategy used by each player based on the provided chess moves. - d4 d5 Nf3 Nf6 Bf4 Bf5 c4 e6 e3 Bb4+ Nc3 O-O Qb3 c5 Be2 dxc4 Bxc4 Nc6 dxc5 Bxc5 Qxb7 Na5 Qb5 Nxc4 Qxc4 Qa5 O-O Rac8 Qb3 Rfd8 Rfd1 Rxd1+ Rxd1 Bg4 a3 Bxf3 gxf3 h6 Qc4\n",
      "### Context: \n",
      "None\n",
      "### Response: ## Game Analysis\n",
      "\n",
      "1. **White: d4, Black: d5**\n",
      "   - **White's move (d4)**: Common opening move controlling the center and allowing the development of the bishop and queen.\n",
      "   - **Black's move (d5)**: Symmetrical response, fighting for central control.\n",
      "\n",
      "2. **White: Nf3, Black: Nf6**\n",
      "   - **White's move (Nf3)**: Develops the knight, attacking the d5 pawn and preparing to castle.\n",
      "   - **Black's move (Nf6)**: Develops the knight, defends the d5 pawn, and prepares to castle.\n",
      "\n",
      "3. **White: Bf4, Black: Bf5**\n",
      "   - **White's move (Bf4)**: Develops the bishop to an active square, supporting the d-pawn and planning to play e3.\n",
      "   - **Black's move (Bf5)**: Mirrors White's move, developing the bishop to an active square where it supports the d5 pawn.\n",
      "\n",
      "4. **White: c4, Black: e6**\n",
      "   - **White's move (c4)**: Challenges the d5 pawn and aims to gain more central control.\n",
      "   - **Black's move (e6)**: Prepares to solidify the pawn structure and develop the light-squared bishop.\n",
      "\n",
      "5. **White: e3, Black: Bb4+**\n",
      "   - **White's move (e3)**: Solidification of the pawn structure and preparing for smooth development.\n",
      "   - **Black's move (Bb4+)**: Developing the bishop aggressively to pin the knight.\n",
      "\n",
      "6. **White: Nc3, Black: O-O**\n",
      "   - **White's move (Nc3)**: Develops the knight to block the check, increasing control over central squares.\n",
      "   - **Black's move (O-O)**: Castles, ensuring king safety and bringing the rook into play.\n",
      "\n",
      "7. **White: Qb3, Black: c5**\n",
      "   - **White's move (Qb3)**: Places pressure on the b7 and d5 pawns, potentially threatening tactical opportunities.\n",
      "   - **Black's move (c5)**: Counter-challenges the center and looks to open the game.\n",
      "\n",
      "8. **White: Be2, Black: dxc4**\n",
      "   - **White's move (Be2)**: Simple development move, preparing to castle.\n",
      "   - **Black's move (dxc4)**: Captures the pawn and creates a small imbalance.\n",
      "\n",
      "9. **White: Bxc4, Black: Nc6**\n",
      "   - **White's move (Bxc4)**: Recaptures the pawn, developing the bishop to an active square.\n",
      "   - **Black's move (Nc6)**: Developing the knight to attack the central squares, particularly d4.\n",
      "\n",
      "10. **White: dxc5, Black: Bxc5**\n",
      "    - **White's move (dxc5)**: Captures the pawn, hoping to disrupt Blackâ€™s pawn structure.\n",
      "    - **Black's move (Bxc5)**: Recaptures, maintaining the bishop on an active square.\n",
      "\n",
      "11. **White: Qxb7, Black: Na5**\n",
      "    - **White's move (Qxb7)**: Grabbing a pawn, with potential traps.\n",
      "    - **Black's move (Na5)**: Attacking the queen, hoping to gain tempo and counterplay.\n",
      "\n",
      "12. **White: Qb5, Black: Nxc4**\n",
      "    - **White's move (Qb5)**: Retreats the queen to a safe square with potential to still be menacing.\n",
      "    - **Black's move (Nxc4)**: Captures the pawn, continuing to gain material.\n",
      "\n",
      "13. **White: Qxc4, Black: Qa5**\n",
      "    - **White's move (Qxc4)**: Recaptures the knight, neutralizing the threat.\n",
      "    - **Black's move (Qa5)**: Pinning the knight and applying pressure on c3.\n",
      "\n",
      "14. **White: O-O, Black: Rac8**\n",
      "    - **White's move (O-O)**: Castles, ensuring king safety.\n",
      "    - **Black's move (Rac8)**: Piles up on the c-file, eyeing the c3 knight.\n",
      "\n",
      "15. **White: Qb3, Black: Rfd8**\n",
      "    - **White's move (Qb3)**: Moves the queen to a more flexible position, supporting b2.\n",
      "    - **Black's move (Rfd8)**: Doubles rooks on the d-file, applying pressure.\n",
      "\n",
      "16. **White: Rfd1, Black: Rxd1+**\n",
      "    - **White's move (Rfd1)**: Contesting the d-file.\n",
      "    - **Black's move (Rxd1+)**: Exchanges rooks to reduce White's control in the center.\n",
      "\n",
      "17. **White: Rxd1, Black: Bg4**\n",
      "    - **White's move (Rxd1)**: Recaptures the rook, maintaining control of the file.\n",
      "    - **Black's move (Bg4)**: Develops the bishop with threat potential.\n",
      "\n",
      "18. **White: a3, Black: Bxf3**\n",
      "    - **White's move (a3)**: Prevents any knight jump to b4 or the bishop from retreating comfortably.\n",
      "    - **Black's move (Bxf3)**: Capturing the knight to weaken Whiteâ€™s pawn structure.\n",
      "\n",
      "19. **White: gxf3, Black: h6**\n",
      "    - **White's move (gxf3)**: Recaptures, opening up the kingside.\n",
      "    - **Black's move (h6)**: Prepares back rank escape and prevents any bishop or knight moves to g5.\n",
      "\n",
      "20. **White: Qc4**\n",
      "    - **White's move (Qc4)**: Positioning the queen on a safer diagonal, possibly aiming for c7.\n",
      "\n",
      "## Summary of Strategy Used and Winner\n",
      "\n",
      "### Strategies Used:\n",
      "- **Central Control**: Both players focused on controlling the center, which is critical for launching attacks and defense.\n",
      "- **Development**: Each side developed their pieces efficiently, striving for optimal placement and connectivity.\n",
      "- **King Safety**: Early castling by both sides ensured the kings were less vulnerable to immediate attacks.\n",
      "- **Tactical Opportunities**: White capitalized on material gain with Qxb7 but later had to carefully defend due to tactical threats like Black's Na5.\n",
      "\n",
      "### Winner:\n",
      "The analysis stops at **Whiteâ€™s move Qc4**. Given the balanced position and the continuous strategic play from both sides, neither player has a decisive advantage. Both sides have strengths, such as White having strong central pawns and active pieces, while Black has rook alignment on open files and better pawn structure on the king's side. Thus, at this point, the game is still open, and itâ€™s premature to declare a winner without further moves.</s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecd022-08ab-4266-b7c8-c433f8d9b978",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25413e2f-82e9-45c7-b9fe-4265e724dad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6a96ed4-c3c4-4130-89aa-4cfa5469bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lora_alpha - scaling factor applied to the low-rank matrices. It helps in balancing the contribution of the low-rank update to the original weights. \n",
    "Higher values of lora_alpha can increase the influence of the low-rank updates. It's a form of regularization to ensure the model doesn't deviate too much from the original weights.\n",
    "\n",
    "bias - \"none\", \"all\", or \"lora_only\".\n",
    "need more research on this.\n",
    "\n",
    "'''\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16, \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eada9b-37ec-45d1-8f45-25ff358baa54",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c17de554-89ef-4f6e-a16e-4b587a3bdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41287cb9-7c68-42fb-9ff7-e32a7c16b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 16:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-669efc05-13f67f5c676de7c873eaecfc;af35edbe-8001-4dc7-a9a4-c8a0a732557e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-v0.1.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-669efd58-2348205d7d5174251b824ea6;b7ed53ed-9293-483c-a33c-620ec152371b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-v0.1.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-669efeab-4265dcfb2ac2748779a02fb2;bfdd86f1-8359-4e9d-856b-c0508cdad914)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-v0.1.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57, training_loss=0.6536283994975843, metrics={'train_runtime': 1015.9703, 'train_samples_per_second': 0.446, 'train_steps_per_second': 0.056, 'total_flos': 4.004818228504166e+16, 'train_loss': 0.6536283994975843, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_args = TrainingArguments(\n",
    "    output_dir=\"mistral_7b_sub_df\",\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\", # apparently more efficient for 32 bit GPUs\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    args=model_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ef1e5-8832-457e-86dc-88dbab1e3085",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0a820d6-0bf8-4e99-88d2-d70a7dca2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = '''[INST] Assume you are a chess master, explain the strategy used by each player based on the provided chess moves. Here are the chess moves in Algenraic Notaion - e4 e6 d4 b6 e5 Bb7 Nf3 h6 Bd3 g5 O-O g4 Nfd2 h5 Ne4 Nc6 Be3 Qe7 Qd2 Bh6 Bxh6 Nxh6 Nf6+ Kd8 Bh7 Nf5 Bxf5 exf5 c3 h4 Qg5 g3 fxg3 hxg3 Qxg3 Qf8 Rxf5 Ne7 Rg5 Ng6 Nd2 Qh6 Rh5 Qg7 Qg4 Bc8 Rxh8+ Qxh8 Rf1 d6 Qg5 Qh4 Qe3 Bb7 e6 [/INST]'''\n",
    "# test_prompt = '''### Question:\n",
    "# What is the capital of Nepal?\n",
    "# ### Context: \n",
    "# ### Response:\n",
    "# '''\n",
    "test_prompt = '''### Question : Who discovered the Ruy Lopez opening?\n",
    "### Context: This opening is popularly known as the Spanish game and was named after a\n",
    "Spanish priest, Ruy Lopez, who discovered this opening in the year of 1561. This\n",
    "opening was however not appreciated or used much at that point of time. Only\n",
    "over the years, this has become a favorite among pros (grandmaster levels as well)\n",
    "and is regarded as one of the most powerful chess openings. It is used as Whiteâ€™s\n",
    "best attempt in gaining an advantage after double king pawn formations. A major\n",
    "plus of this opening is that it gives the white player enough opportunity to develop\n",
    "a complex offensive strategy and also slows down Blackâ€™s pawn formation.\n",
    "### Response: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df1f8bd5-b98b-4d42-8cd2-38d464bab867",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    add_bos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84a3a6bc-0eb3-4882-8859-df7e1f14a749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   774, 22478,   714,  6526,  8324,   272,   399,  4533,   393,\n",
       "          1845, 28764,  7032, 28804,    13, 27332, 14268, 28747,   851,  7032,\n",
       "           349,  4387,   346,  2651,   390,   272, 10177,  2039,   304,   403,\n",
       "          5160,  1024,   264,    13, 13116,   789, 16032, 28725,   399,  4533,\n",
       "           393,  1845, 28764, 28725,   693,  8324,   456,  7032,   297,   272,\n",
       "           879,   302, 28705, 28740, 28782, 28784, 28740, 28723,   851,    13,\n",
       "           410,  3250,   403,  3545,   459, 22359,   442,  1307,  1188,   438,\n",
       "           369,  1305,   302,   727, 28723,  6352,    13,  1483,   272,  1267,\n",
       "         28725,   456,   659,  2727,   264,  6656,  3352, 14138,   325, 24433,\n",
       "          9548,  6157,   390,  1162, 28731,    13,   391,   349, 15390,   390,\n",
       "           624,   302,   272,  1080,  6787,   997,   819,  1565,   742, 28723,\n",
       "           661,   349,  1307,   390,  5673, 28809, 28713,    13, 13521,  4236,\n",
       "           297, 25221,   396,  7859,  1024,  3579,  6779,  5881,   880,  1221,\n",
       "           697, 28723,   330,  3014,    13, 11269,   302,   456,  7032,   349,\n",
       "           369,   378,  5212,   272,  3075,  4385,  2066,  5701,   298,  1950,\n",
       "            13, 28708,  4630, 17381,  7213,   304,   835,  1135,  3611,  1060,\n",
       "          4777, 28809, 28713,  5881,   880, 11515, 28723,    13, 27332, 12107,\n",
       "         28747, 28705]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = eval_tokenizer(test_prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46796721-6d25-46f2-86cc-9f93fd615f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer(test_prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b18a82f3-85f4-483e-919d-499dc864cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question : Who discovered the Ruy Lopez opening?\n",
      "### Context: This opening is popularly known as the Spanish game and was named after a\n",
      "Spanish priest, Ruy Lopez, who discovered this opening in the year of 1561. This\n",
      "opening was however not appreciated or used much at that point of time. Only\n",
      "over the years, this has become a favorite among pros (grandmaster levels as well)\n",
      "and is regarded as one of the most powerful chess openings. It is used as Whiteâ€™s\n",
      "best attempt in gaining an advantage after double king pawn formations. A major\n",
      "plus of this opening is that it gives the white player enough opportunity to develop\n",
      "a complex offensive strategy and also slows down Blackâ€™s pawn formation.\n",
      "### Response: \n",
      "The context mentions that the Ruy Lopez opening was discovered by a Spanish priest named Ruy Lopez in the year 1561. The opening was not widely used or appreciated at that time but has become a favorite among pros and is considered one of the most powerful openings. It is particularly effective for White when aiming for a double king pawn formation.\n",
      "\n",
      "Therefore, the answer to the question \"Who discovered the Ruy Lopez opening?\" is Ruy Lopez.\n",
      "\n",
      "<ANSWER>: Ruy Lopez\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask = torch.where(input_ids == 2, 0, 1),\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633417b-6e08-4b01-ae56-92ab050a936c",
   "metadata": {},
   "source": [
    "## saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f05115b5-207e-45e5-bb04-cdb5251fea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-669efeb6-26fae5b47a76596271b7e3d6;7254235c-c2bb-492c-91c4-ee497df35aad)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-v0.1.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713ac04-58b6-47f4-bb1b-ff5aaacd3992",
   "metadata": {},
   "source": [
    "## saving to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea69a9ac-4db4-43f1-b39e-d138dfb49f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a64c43e5-498a-476f-83e6-7b5d0163de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"./mistral_7b/\", device_map=\"cuda:0\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8f87c70-c140-4258-9056-5044c5900f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45727179-750a-47f4-b2d7-8968298090a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-669efeb8-2455f8a561f5667e184618bd;3cbc98ce-742d-42db-b4fe-dc7c23d4ac20)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-v0.1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2de617e566544b58a85f73419ed7267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/OpenSI/cognitive_AI/commit/5c0ff4bd7e12455e8a7f21fcad1adba86ee29c03', commit_message='Upload model', commit_description='', oid='5c0ff4bd7e12455e8a7f21fcad1adba86ee29c03', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"OpenSI/cognitive_AI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
