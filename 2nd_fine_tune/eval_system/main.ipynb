{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d723069d-ac49-4c49-99f9-8d5f2f4c8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler('eval.log', mode=\"a\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f809bd9e-74f0-4ac5-8017-406e40a9111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 00:34:26,977 - __main__ - INFO - 1 tests found.\n",
      "2024-07-23 00:34:26,980 - __main__ - INFO - Tests: ['memory']\n"
     ]
    }
   ],
   "source": [
    "folders = [name for name in os.listdir('.') if os.path.isdir(name) and name[0]!= \".\"]\n",
    "logger.info(f\"{len(folders)} test(s) found.\")\n",
    "logger.info(f\"Tests: {folders}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35726ed-f8cf-493e-a678-65a4dbed8e93",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e955c64e-7893-45a7-8f58-3e83aa9a0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 00:37:49,913 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 00:37:50,166 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/whoami-v2 HTTP/11\" 200 389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/s448780/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv(\"../../finetune/.env\")\n",
    "login(os.getenv(\"hf_token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82928cda-e9ba-40db-96f0-e2d8dd0a3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"finetuned\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"lora_adapter\" : \"OpenSI/cognitive_AI\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15e80600-1f9b-4b7a-852e-225331fc60a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 00:48:26,152 - urllib3.connectionpool - DEBUG - Resetting dropped connection: huggingface.co\n",
      "2024-07-23 00:48:28,358 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/config.json HTTP/11\" 200 0\n",
      "2024-07-23 00:48:28,833 - bitsandbytes.cextension - DEBUG - Loading bitsandbytes native library from: /home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "2024-07-23 00:48:29,429 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bfdd830f8d406b9af028c715f104dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 00:48:36,025 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "2024-07-23 00:48:36,829 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = models[\"finetuned\"]\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "                                          add_bos_token = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f62e68-5ece-4999-8d05-8a9eb0522fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 00:53:46,141 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /OpenSI/cognitive_AI/resolve/main/adapter_config.json HTTP/11\" 200 0\n",
      "2024-07-23 00:53:53,170 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /OpenSI/cognitive_AI/resolve/main/adapter_config.json HTTP/11\" 200 0\n",
      "2024-07-23 00:53:55,149 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /OpenSI/cognitive_AI/resolve/main/adapter_config.json HTTP/11\" 200 0\n",
      "2024-07-23 00:53:55,184 - peft.tuners.tuners_utils - INFO - Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "2024-07-23 00:53:58,128 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /OpenSI/cognitive_AI/resolve/main/adapter_model.safetensors HTTP/11\" 302 0\n",
      "2024-07-23 00:54:00,903 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /OpenSI/cognitive_AI/resolve/main/adapter_model.safetensors HTTP/11\" 302 0\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "lora = models[\"lora_adapter\"]\n",
    "adapter_config = PeftConfig.from_pretrained(lora)\n",
    "\n",
    "model_f = PeftModel.from_pretrained(model, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55dcadb0-a737-440e-9602-2e91190c5e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53097e2b-fa0c-4479-8888-5d53d5027219",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2f80678-a591-4a68-96fa-afcfa4be4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''### Question: What is the name of the piece that moves in an L shape?\n",
    "        ## Response:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8213adf0-c125-4358-9284-c95b415d0a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   774, 22478, 28747,  1824,   349,   272,  1141,   302,   272,\n",
       "          5511,   369, 11662,   297,   396,   393,  5843, 28804,    13,  5390,\n",
       "           531, 12107, 28747]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30e1c14a-5f10-4adc-9c11-3b4fc8e8aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the name of the piece that moves in an L shape?\n",
      "        ## Response: The piece that moves in an L shape is called a knight.\n",
      "\n",
      "Here's why:\n",
      "- The knight is unique among chess pieces in that it moves in an L shape: two squares in one direction and then one square perpendicular to that.\n",
      "- This unusual move makes the knight particularly powerful, as it can jump over other pieces and quickly reposition itself on the board.\n",
      "\n",
      "Therefore, the piece that moves in an L shape is called a knight.\n",
      "\n",
      "<ANSWER>: Knight\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask = torch.where(input_ids == 2, 0, 1),\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b496daf-f4b0-4eba-bc19-1b2b1bc5e5ec",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "437601d6-51e8-4bb2-ac54-599cedd8a3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the piece that moves in an...</td>\n",
       "      <td>Knight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many squares are there on a standard chess...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the term for a situation where a king ...</td>\n",
       "      <td>Checkmate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which piece is the most powerful in chess?</td>\n",
       "      <td>Queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the initial position of the white queen?</td>\n",
       "      <td>d1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question     Answer\n",
       "0  What is the name of the piece that moves in an...     Knight\n",
       "1  How many squares are there on a standard chess...         64\n",
       "2  What is the term for a situation where a king ...  Checkmate\n",
       "3         Which piece is the most powerful in chess?      Queen\n",
       "4   What is the initial position of the white queen?         d1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "memory = pd.read_csv(\"../../data/test_framework/memory/memory.csv\")\n",
    "memory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6adc7-f388-4318-9aa2-5414d43fa182",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_questions = memory[\"Question\"]\n",
    "memory_answers = memory[\"Answer\"]\n",
    "\n",
    "memory_result_finetuned = pd.DataFrame(columns=[\"Question\", \"GT\", \"Answer\", \"Score\"])\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for i, question in tqdm(enumerate(memory_questions)):\n",
    "    gt = memory_answers[i]\n",
    "    prompt = f'''### Question: {question}\n",
    "    ## Response:\n",
    "    '''\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask = torch.where(input_ids == 2, 0, 1),\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=True, \n",
    "            top_p=0.9,\n",
    "            temperature=0.5)\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(question)\n",
    "        print(answer)\n",
    "        print(\"-\"*10)\n",
    "        memory_result_finetuned.loc[memory_result_finetuned.shape[0]] = {\"Question\" : question, \"GT\" : gt, \"Answer\" : answer, \"Score\" : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9217f0f-690e-4912-9f8f-4b6d5083778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_result_finetuned.to_csv(\"memory_ft.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
