{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cee869-9d96-4320-8745-2918f8666188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login, login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5250f8-dc34-4eb3-99e7-e5fa5f251801",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2000f7be-d2ff-474f-9bc4-7378d64383d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/s448780/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5483d5-9f69-4e2d-9e1e-52ef4a7de5e0",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8259f4c-4baa-44ae-b60b-2dabe4b4c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14cfc702-d836-4b74-8e55-953924106b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e98cf66d7f455da0d543b7a834e732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# load model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.pretraining_tp = 1 #parallel GPU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519c0ac-5701-4a69-8d84-91690a0416fe",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8ffe7a-8201-4bde-a6ca-75cb41db7edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['moves', 'explanation', 'instruction'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./filtered_data/diverse_data.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86194a2b-eff7-4ff7-9af1-69f3fd5f9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_instruction(sample):\n",
    "#     return f\"\"\"You are a chess expert. Explain the rationale behind the last move from the given chess moves in Algebraic notation - \n",
    "#         {sample[\"moves\"]}\n",
    "\n",
    "#         ### Response:\n",
    "#         {sample[\"explanation\"]}\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c8540d-a3d4-44d9-bdfb-b37381504d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset[\"train\"][0]\n",
    "# print(format_instruction(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15030b0b-f338-487d-bf06-5e8ee120b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_row(instruction, moves, explanation):\n",
    "    return f\"\"\"<s>[INST] {instruction}\\nHere are the chess moves in Algenraic Notaion - {moves} [/INST]\\n{explanation} </s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d6c5f5-b7ef-4243-932f-e58ce4a788d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_func(df):\n",
    "    instructions = df[\"instruction\"]\n",
    "    inputs      = df[\"moves\"]\n",
    "    outputs     = df[\"explanation\"]\n",
    "    texts = []\n",
    "    for instruction, inp, output in zip(instructions, inputs, outputs):\n",
    "        text = create_text_row(instruction, inp, output) # eos token coming from create_text_row function\n",
    "        #print(text)\n",
    "        texts.append(text)\n",
    "    return {\"text\" : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3575e432-ab90-4cc3-aed2-b19dc1231e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69457ab68f0f4e7a8b20a7ff3a62e565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee7265c7-a901-48df-8768-fc7034d1b734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['moves', 'explanation', 'instruction', 'text'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4453245a-5b50-46ef-b2da-2e5da3c8b645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Assume you are a chess master. Who do you think will win the game based on the provided chess moves in Algebraic Notation.\n",
      "Here are the chess moves in Algenraic Notaion - e4 e6 d4 d5 Nd2 c5 exd5 exd5 dxc5 Bxc5 Nb3 Bb6 Nf3 Nf6 Bd3 Nc6 c3 h6 O-O O-O Bf4 Be6 Qd2 Ne4 Qc2 f5 Nbd4 Nxd4 Nxd4 Bd7 Rfe1 Rc8 Qb3 Nc5 Qxd5+ Kh8 Bc2 Ne4 Bxe4 fxe4 Qxe4 Bxd4 Qxd4 Bc6 Qxd8 Rcxd8 Bg3 Rd2 b4 Rc2 Re3 Rd8 a3 Rdd2 h4 g6 Rae1 Ra2 c4 a6 c5 Kg8 Kf1 Bb5+ Kg1 Bc6 Re6 Kf7 R1e3 h5 Rd6 Re2 Rxe2 Rxe2 f3 Ra2 Rd3 Ke6 Kf1 Bb5 c6 Bxd3+ Kg1 bxc6 [/INST]\n",
      "White. The game dynamics favored White, who effectively transitioned from a sound opening into an actively simplified endgame, converting piece activity and centralized power into a winning material advantage. </s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecd022-08ab-4266-b7c8-c433f8d9b978",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25413e2f-82e9-45c7-b9fe-4265e724dad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a96ed4-c3c4-4130-89aa-4cfa5469bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lora_alpha - scaling factor applied to the low-rank matrices. It helps in balancing the contribution of the low-rank update to the original weights. \n",
    "Higher values of lora_alpha can increase the influence of the low-rank updates. It's a form of regularization to ensure the model doesn't deviate too much from the original weights.\n",
    "\n",
    "bias - \"none\", \"all\", or \"lora_only\".\n",
    "need more research on this.\n",
    "\n",
    "'''\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16, \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eada9b-37ec-45d1-8f45-25ff358baa54",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c17de554-89ef-4f6e-a16e-4b587a3bdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41287cb9-7c68-42fb-9ff7-e32a7c16b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a2d0e18e7e423f9c2075e12d768149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 14:48, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.800800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=0.8929822211172066, metrics={'train_runtime': 906.5274, 'train_samples_per_second': 0.457, 'train_steps_per_second': 0.056, 'total_flos': 3.571625969791795e+16, 'train_loss': 0.8929822211172066, 'epoch': 2.914285714285714})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_args = TrainingArguments(\n",
    "    output_dir=\"mistral_7b\",\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\", # apparently more efficient for 32 bit GPUs\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    args=model_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ef1e5-8832-457e-86dc-88dbab1e3085",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff0b62e-3159-4f3e-aedc-e1678832f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_input = \"Explain the rationale behind the last move from the given chess moves in Algebraic notation -\"\n",
    "# test_moves = \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4\"\n",
    "# test_prompt = f\"\"\" {test_input}\n",
    "#         ### Input:\n",
    "#         {test_moves}\n",
    "\n",
    "#         ### Response:\n",
    "\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0a820d6-0bf8-4e99-88d2-d70a7dca2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = '''\n",
    "[INST] Assume you are a chess master, explain the strategy used by each player based on the provided chess moves. Here are the chess moves in Algenraic Notaion - e4 e6 d4 b6 e5 Bb7 Nf3 h6 Bd3 g5 O-O g4 Nfd2 h5 Ne4 Nc6 Be3 Qe7 Qd2 Bh6 Bxh6 Nxh6 Nf6+ Kd8 Bh7 Nf5 Bxf5 exf5 c3 h4 Qg5 g3 fxg3 hxg3 Qxg3 Qf8 Rxf5 Ne7 Rg5 Ng6 Nd2 Qh6 Rh5 Qg7 Qg4 Bc8 Rxh8+ Qxh8 Rf1 d6 Qg5 Qh4 Qe3 Bb7 e6 [/INST]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84a3a6bc-0eb3-4882-8859-df7e1f14a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b18a82f3-85f4-483e-919d-499dc864cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] Assume you are a chess master, explain the strategy used by each player based on the provided chess moves. Here are the chess moves in Algenraic Notaion - e4 e6 d4 b6 e5 Bb7 Nf3 h6 Bd3 g5 O-O g4 Nfd2 h5 Ne4 Nc6 Be3 Qe7 Qd2 Bh6 Bxh6 Nxh6 Nf6+ Kd8 Bh7 Nf5 Bxf5 exf5 c3 h4 Qg5 g3 fxg3 hxg3 Qxg3 Qf8 Rxf5 Ne7 Rg5 Ng6 Nd2 Qh6 Rh5 Qg7 Qg4 Bc8 Rxh8+ Qxh8 Rf1 d6 Qg5 Qh4 Qe3 Bb7 e6 [/INST]\n",
      "Black adopted the King's Indian Defense to counter White's opening moves, aiming to create imbalances and potential counterplay on the queenside. White capitalized on the open lines and central control to mount a strong pawn attack, with the eventual goal of promoting pawns on the queenside. The game showcased an aggressive and strategic approach from White, leveraging central control and potential queenside play, while Black aimed to create counterplay and leverage the King's Indian Defense's inherent weaknesses. The game is not fully concluded, but White has established a strong attacking position, particularly with the advancing pawns and active queen on the queenside. \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c2317-c9fa-4e60-976f-83bf73dc082e",
   "metadata": {},
   "source": [
    "## testing base knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d67d11-04f8-448d-a947-eecd09d56a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]You will be given the name of a chess piece. Explain how the chess piece can move -\n",
      "Knight.[/INST]\n",
      "The Knight can move to a square that is two squares away in one direction and one square in the other, as long as the squares are not of the same color. The Knight can also jump over other pieces, but cannot move to a square that is already occupied by a piece of the same color.\n"
     ]
    }
   ],
   "source": [
    "test_prompt = '''\n",
    "[INST]You will be given the name of a chess piece. Explain how the chess piece can move -\n",
    "Knight.[/INST]\n",
    "'''\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9)\n",
    "    \n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c6b43b-3ea0-4811-8c55-62e270aa9fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]Assume you are chess master. Who do you think will win the game based on the provided chess moves from a real game -\n",
      "d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4.[/INST]\n",
      "White. \n"
     ]
    }
   ],
   "source": [
    "test_prompt = '''\n",
    "[INST]Assume you are chess master. Who do you think will win the game based on the provided chess moves from a real game -\n",
    "d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4.[/INST]\n",
    "'''\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9)\n",
    "    \n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6ab97e-79c5-4249-8a79-3931a154b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]Assume you are a geography expert. You will be provided with the name of a country. What's the capital of the country?\",\n",
      "Australia[/INST].\n",
      "Canberra\n",
      "\n",
      "# Canberra\n",
      "Canberra is the capital city of Australia. It is located in the Australian Capital Territory (ACT), an area of land within New South Wales that was granted self-government in 1988.\n",
      "\n",
      "Canberra was designed as the capital of Australia in 1908 by a competition held by the Australian government. It was named after Lady Canberra, the wife of the Duke of Albany, who was also a close friend of Australia's first Prime Minister, Edmund Barton.\n",
      "\n",
      "Canberra is home to a number of important government buildings, such as Parliament House, the Australian War Memorial, and the National Gallery of Australia. It is also home to several universities, including the Australian National University and the University of Canberra.\n",
      "\n",
      "Canberra has a population of around 400,000 people, making it the eighth largest city in Australia. It is a popular tourist destination, known for its clean air, beautiful scenery, and diverse range of attractions.\n",
      "\n",
      "# Summary\n",
      "Canberra is the capital city of Australia and is located in the Australian Capital Territory. It was designed as the capital of Australia in 1908, and it is home to several important government buildings and universities. Canberra has a population of around 400,000 people and is a popular tourist destination. The capital of Australia is Canberra. \n"
     ]
    }
   ],
   "source": [
    "test_prompt ='''\n",
    "[INST]Assume you are a geography expert. You will be provided with the name of a country. What's the capital of the country?\",\n",
    "Australia[/INST].\n",
    "'''\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9)\n",
    "    \n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f966b89b-1fae-4072-944f-f6ed9401af73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Assume you are a chess master. Your job is to suggest the best next move based on a list of previous chess moves. Here are the moves - d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4[\\INST]\n",
      "    Nf6 \n"
     ]
    }
   ],
   "source": [
    "test_input = \"Assume you are a chess master. Your job is to suggest the best next move based on a list of previous chess moves.\"\n",
    "test_moves = \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4\"\n",
    "test_prompt = f\"[INST]{test_input} Here are the moves - {test_moves}[\\INST]\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "997b3b9c-1edc-43f9-865a-c7810656ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Assume you are a chess master. Analyse each move pair and explain the players' strategy. Here are the moves - d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4[\\INST]\n",
      "\n",
      "### Game analysis\n",
      "\n",
      "1. **White: d4, Black: d5**  \n",
      "   - **White:** Opens with the Queen's Pawn Opening, controlling the center and aiming to develop the bishop.\n",
      "   - **Black:** Responds symmetrically to control the center and prepare to develop the bishop.\n",
      "\n",
      "2. **White: c4, Black: c6**  \n",
      "   - **White:** Advances the Queen's Pawn to support the d4 pawn and prepares for a potential cxd5 exchange.\n",
      "   - **Black:** Plays c6 to prepare to challenge the center with ...dxc4 and avoids being pinned by a potential Nf3-g5.\n",
      "\n",
      "3. **White: cxd5, Black: e6**  \n",
      "   - **White:** Exchanges pawns to disrupt Black’s central control and open lines for the bishop and queen.\n",
      "   - **Black:** Advances the e-pawn to prepare to recapture the d-pawn and to control more central space.\n",
      "\n",
      "4. **White: dxe6, Black: fxe6**  \n",
      "   - **White:** Captures the pawn, opening the f-file and trying to create a passed pawn.\n",
      "   - **Black:** Recaptures the pawn to maintain central control and opens lines for the bishop and queen.\n",
      "\n",
      "5. **White: Nf3, Black: Bb4+**  \n",
      "   - **White:** Develops a knight to support the d-pawn and control the center.\n",
      "   - **Black:** Checks with the bishop to challenge White’s ability to castle and develop pieces.\n",
      "\n",
      "6. **White: Nc3, Black: Ba5**  \n",
      "   - **White:** Blocks the check and develops another knight to exert more control over the center.\n",
      "   - **Black:** Retreats the bishop to a safe square where it can potentially control the c4 pawn and support future central activity.\n",
      "\n",
      "7. **White: Bf4, Black: Nf6**  \n",
      "   - **White:** Develops the bishop to a strong square, aiming at the queen-side and exerting pressure on the e4 square.\n",
      "   - **Black:** Develops the knight to control the center and prepare to support e5 or other central breaks.\n",
      "\n",
      "### Strategies used:\n",
      "1. **Central Control:** Both players aim to control the center, which is essential for a strong opening.\n",
      "2. **Development:** Both sides prioritize early development of their minor pieces to be ready for the middlegame.\n",
      "3. **Tactical Play:** Black utilizes Bb4+ to disrupt White’s plans and gain a small advantage.\n",
      "\n",
      "### Winner:\n",
      "The provided sequence of moves doesn't lead to a definitive winner. Both sides have strengths and weaknesses, with Black having a slight edge due to the effective Bb4+ check. However, neither side has achieved a decisive strategic advantage yet. Further moves would be required to determine the winner or declare a draw.\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Assume you are a chess master. Analyse each move pair and explain the players' strategy.\"\n",
    "test_moves = \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4\"\n",
    "test_prompt = f\"[INST]{test_input} Here are the moves - {test_moves}[\\INST]\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633417b-6e08-4b01-ae56-92ab050a936c",
   "metadata": {},
   "source": [
    "## saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f05115b5-207e-45e5-bb04-cdb5251fea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
