{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/s448780/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login, login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../finetune/.env\")\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d37177fcbd546b4bbd52c91701846f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model_name = \"adnaan525/opensi_mistral_3tasks\"\n",
    "adapter_config = PeftConfig.from_pretrained(lora_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='mistralai/Mistral-7B-v0.1', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=32, target_modules={'gate_proj', 'down_proj', 'o_proj', 'k_proj', 'q_proj', 'v_proj', 'up_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_lora = PeftModel.from_pretrained(model, lora_model_name)\n",
    "model_with_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with model_with_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Assume you are a chess master. Analyse each move pair and explain the players' strategy. Here are the moves - d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5[\\INST]\n",
      "\n",
      "## Game Analysis:\n",
      "\n",
      "### Move Pair 1: White: d4, Black: d5\n",
      "**White: d4** - White opens with the Queen's Pawn, aiming to control the center.\n",
      "**Black: d5** - Black responds symmetrically, contesting the center.\n",
      "\n",
      "### Move Pair 2: White: c4, Black: c6\n",
      "**White: c4** - White continues with the Queen's Gambit, trying to gain more space in the center.\n",
      "**Black: c6** - Black prepares to fianchetto the bishop, looking to control the long diagonal.\n",
      "\n",
      "### Move Pair 3: White: cxd5, Black: e6\n",
      "**White: cxd5** - White captures the pawn, disrupting Black's pawn structure.\n",
      "**Black: e6** - Black recaptures the pawn, keeping the pawn structure intact and preparing for potential future development.\n",
      "\n",
      "### Move Pair 4: White: dxe6, Black: fxe6\n",
      "**White: dxe6** - White sacrifices the pawn to disrupt Black's pawn structure further.\n",
      "**Black: fxe6** - Black recaptures, maintaining the pawn structure but with weaker pawns.\n",
      "\n",
      "### Move Pair 5: White: Nf3, Black: Bb4+\n",
      "**White: Nf3** - White develops the knight, attacking the e6 pawn and preparing for kingside castling.\n",
      "**Black: Bb4+** - Black checks with the bishop, forcing White to react and potentially gain a tempo.\n",
      "\n",
      "### Move Pair 6: White: Nc3, Black: Ba5\n",
      "**White: Nc3** - White blocks the check, developing the knight and controlling the central squares.\n",
      "**Black: Ba5** - Black retreats the bishop, keeping it active and aiming for future kingside castling.\n",
      "\n",
      "## Summary of Strategies Used and Winner\n",
      "\n",
      "- **Strategies Used:**\n",
      "  - **Central Control:** Both players strive to control the center of the board with pawns and pieces.\n",
      "  - **Development:** Both sides focus on early development of minor pieces to exert control over the board.\n",
      "  - **Pawn Structure:** White sacrifices a pawn to disrupt Black’s pawn structure and gain an advantage.\n",
      "  - **Tactical Play:** Black's bishop check on move 5 highlights the importance of tactical considerations.\n",
      "\n",
      "- **Winner:** This position is not conclusive for determining a winner as further moves are required to evaluate the advantage. However, White has gained an initial edge through the tactical sacrifice, though Black’s responses show promise to regain control. The game continues with further moves, and the winner will depend on future developments and precise play. \n"
     ]
    }
   ],
   "source": [
    "test_input = \"Assume you are a chess master. Analyse each move pair and explain the players' strategy.\"\n",
    "test_moves = \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5\"\n",
    "test_prompt = f\"[INST]{test_input} Here are the moves - {test_moves}[\\INST]\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model_with_lora.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='mistralai/Mistral-7B-v0.1', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=32, target_modules={'gate_proj', 'down_proj', 'o_proj', 'k_proj', 'q_proj', 'v_proj', 'up_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_lora_adapters(model):\n",
    "    lora_adapters = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraConfig):\n",
    "            lora_adapters.append(name)\n",
    "    return lora_adapters\n",
    "\n",
    "# Get the list of LoRA adapters\n",
    "lora_adapters = list_lora_adapters(model_with_lora)\n",
    "print(\"List of LoRA adapters:\", lora_adapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model:   0%|                 | 0/678 [00:00<?, ?it/s]/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Unloading and merging model: 100%|███████| 678/678 [00:17<00:00, 38.73it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_model = model_with_lora.merge_and_unload(progressbar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Assume you are a chess master. Analyse each move pair and explain the players' strategy. Here are the moves - d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5[\\INST]\n",
      "\n",
      "[QUI]What move would you make to stop the game? [\\QUI]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[QUI]Who is the stronger player in this game? [QUI]\n",
      "\n",
      "[BRE]The next move by the white player is... [BRE]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[QUI]Is this a good move? [QUI]\n",
      "\n",
      "[BRE]The next move by the white player is... [BRE]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[QUI]Is this a good move? [QUI]\n",
      "\n",
      "[QUI]How would you respond? [QUI]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[QUI]Is this a good move? [QUI]\n",
      "\n",
      "[QUI]How would you respond? [QUI]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[BRE]The next move by the white player is... [BRE]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[BRE]The next move by the white player is... [BRE]\n",
      "\n",
      "[BRE]The next move by the black player is... [BRE]\n",
      "\n",
      "[QUI]Is this a good move? [QUI]\n",
      "\n",
      "[QUI]How would you respond? [QUI]\n",
      "\n",
      "[BRE]The next move by the white player is... [BRE]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [QUI]\n",
      "\n",
      "[QUI]What is the name of the opening? [Q\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Assume you are a chess master. Analyse each move pair and explain the players' strategy.\"\n",
    "test_moves = \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5\"\n",
    "test_prompt = f\"[INST]{test_input} Here are the moves - {test_moves}[\\INST]\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = merged_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing model_with_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Assume you are a chess master. Analyse each move pair and explain the players' strategy. Here are the moves - d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5[\\INST]\n",
      "\n",
      "1) The first two moves show a King's Gambit in which White sacrifices a pawn for quick development. In return, White exchanges the pieces as fast as possible, trying to gain a tempo.\n",
      "2) White now sacrifices a pawn to gain the tempo.\n",
      "3) Black accepts the pawn. White tries to maintain tempo advantage.\n",
      "4) Black tries to get out of this situation.\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Assume you are a chess master. Analyse each move pair and explain the players' strategy.\"\n",
    "test_moves = \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5\"\n",
    "test_prompt = f\"[INST]{test_input} Here are the moves - {test_moves}[\\INST]\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda:0\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model_with_lora.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with llama_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import llama_index\n",
    "from llama_index.core import SimpleDirectoryReader # to load docs\n",
    "from llama_index.core import Document\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate # generate prompt template with role\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_response # formatting\n",
    "\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM # llm\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # embedding\n",
    "\n",
    "from llama_index.core import Settings # pass llm and embedding\n",
    "\n",
    "from llama_index.core import VectorStoreIndex # store vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s448780/miniconda3/envs/pydev/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ucl_2023.pdf'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"./test_doc\").load_data()\n",
    "books = set()\n",
    "for book in documents:\n",
    "  books.add(book.metadata[\"file_name\"])\n",
    "\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"adnaan525/merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Always answer the question, even if the context isn't helpful.\"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "The model `adnaan525/merged` and tokenizer `mistralai/Mistral-7B-v0.1` are different, please ensure that they are compatible.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_name=model_id,\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\" : True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"how many titles does real madrid have? use the most updated information\")\n",
    "display_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
